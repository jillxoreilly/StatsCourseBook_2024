In null hypothesis testing, we mainly think about about $p$-values (1-specificity; controlling type I errors).

We **assume the null is true**, ie the true population effect is zero (say, a correlation is zero)
    * the $p$-value gives the probability that theh observed test statstistic (say, correlation r=0.25) could have arisen due to chance.  
    * the $\alpha$ value is the cut-off $p$-value for the test to be significant, so if we test at the $alpha=0.05$ level, the test will be significant if $p$<0.05
    * Therefore the choice of $\alpha$ controls the number of false positives
    
Conversely, when thinking about **power** (sensitivity; controlling type II errors) we **assume the alternative hypothesis is true**, ie that the true population effect is non-zero.
* The power of the test is then the proportion of times we manage to reject the null hypothesis